import json
import os
import constants
import torch
from pykeen.regularizers import LpRegularizer
from negateive_samplers.wrapper_negative_sampler import WrapperNegativeSampler
from pykeen.sampling import BasicNegativeSampler, BernoulliNegativeSampler
from pykeen.models import TransE
from pykeen.training import SLCWATrainingLoop
from pykeen.evaluation import LCWAEvaluationLoop, RankBasedEvaluator
from pykeen.optimizers import Adam
from pykeen.losses import MarginRankingLoss
from pykeen.stoppers import EarlyStopper
from pykeen.models import Model
import copy
import sys
import time
from datetime import datetime

# my files or classes
import util
import model_loader
from shared_index import SharedIndex
from data_loader import load_data

# IMPORTS FROM KGE-RL
from negateive_samplers.kge_rl import data_loader

def run_experiment(exp_config_path, data_path, experiment_name, dataset_name):

    print("Starting eperiment for:")
    print("\tExperiment config. path: ", exp_config_path)
    print("\tData path: ", data_path)
    print("\tExperiment name: ", experiment_name)
    print("\tDataset name: ", dataset_name)
    print("**********************************************")

    if os.path.exists(os.path.join(data_path,exp_config_path,experiment_name+".json")):
        exp_config = json.load(open(os.path.join(data_path,exp_config_path,experiment_name+".json")))

        if exp_config["neg_sampler"] == "adversarial"
            print("dummy")
            return

        exp_config["is_dev"] = True # TODO: it is hard fixed, later get the value from config
        exp_config["dataset_name"] = dataset_name # dataset name is not give in autogenerated configs, therefore, dataset name is received in parameters
    else:
        print("configuration file does not exist. Exiting")
        return
    
    config = exp_config
     
    config["exp_name"] = config["model"]+"_"+str(config["num_negs"])
    config["results_dir"] =  os.path.join(data_path,"results",config["dataset_name"],config["neg_sampler"])
    #config["data_index_path"] = os.path.join(data_path,"Results",config["dataset_name"]) # for storing entity and relation index in pickle
    config["dataset_path"] = os.path.join(data_path, config["dataset_name"])
 
    print("Process Number: {} for experiment: ".format(os.getpid()))
    print("\tDataset: {}, Model: {}, Num. of Negs: {}".format(config["dataset_name"],config["model"], config["num_negs"]))
    

    os.makedirs(config["results_dir"], exist_ok=True)

    util.dump_json(config, config["results_dir"], "{}_config.json".format(config["exp_name"]))
    
    #Zafar: unecessary step, but I have to load the dataset in structur supported by existing code so that negative sampler can be created
    print("Loading dataset for reusable code..")
    data_set = data_loader.read_dataset(
        config["dataset_path"], 
        config["dataset_path"], # this is result directory where the entity and relation indexes are available in pickle
        #dev_mode= not is_model_test, #TODO: temporarily commented for condor server only (ASSUMING THAT WE ARE ALWAY TRAINING)
        dev_mode= True,
        max_examples=float('inf')
    )

    #IMPORTANT: data_set['test'] contains validation set if dev_mode is true,
    # otherwise, it contains test set only if pre-trained model is loaded and used for testing only
    data_for_negative_sampler = copy.copy(data_set['train'])
    data_for_negative_sampler.extend(data_set['test'])
    
    cuda = torch.cuda.is_available()
    print("CUDA is available: {}".format(cuda))

    train_triples, validation_triples, test_triples, neg_sampler_triples = load_data(data_path, config["dataset_name"], is_dev_mode=True)

# region Pykeen code Beyond Pipeline    
    # train_triples = dataset.training
    # validation_triples = dataset.validation
    # test_triples = dataset.testing


    # # Get a training dataset
    # from pykeen.datasets import Nations
    # dataset = Nations()
    # training_triples_factory = dataset.training

    # # Pick a model
    # from pykeen.models import TransE
    # model = TransE(triples_factory=training_triples_factory)

    # # Pick an optimizer from Torch
    # from torch.optim import Adam
    # optimizer = Adam(params=model.get_grad_params())

    # # Pick a training approach (sLCWA or LCWA)
    # from pykeen.training import SLCWATrainingLoop
    # training_loop = SLCWATrainingLoop(
    #     model=model,
    #     triples_factory=training_triples_factory,
    #     optimizer=optimizer,
    # )

    # # Train like Cristiano Ronaldo
    # _ = training_loop.train(
    #     triples_factory=training_triples_factory,
    #     num_epochs=5,
    #     batch_size=256,
    # )

    # # Pick an evaluator
    # from pykeen.evaluation import RankBasedEvaluator
    # evaluator = RankBasedEvaluator()

    # # Get triples to test
    # mapped_triples = dataset.testing.mapped_triples

    # # Evaluate
    # results = evaluator.evaluate(
    #     model=model,
    #     mapped_triples=mapped_triples,
    #     batch_size=1024,
    #     additional_filter_triples=[
    #         dataset.training.mapped_triples,
    #         dataset.validation.mapped_triples,
    #     ],
    # )
    # print(results)
# endregion 

    _device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    config["device"] = _device

    _regularizer = LpRegularizer(
            p=2, 
            weight=config.get("l2",0.0001),
            )  # L2 regularization with defult weight 0.0001


    _loss = MarginRankingLoss(
        margin=1
        )

    _model = model_loader.build_model(
        config["model"], 
        train_triples, 
        _regularizer,
        _loss,
        config["ent_dim"],
        _device,
        )

    _optimizer_kwargs= dict(lr=config.get("lr",0.01))
    
    # from pykeen.optimizers import get_optimizer
    # optimizer = get_optimizer(
    #     'Adam',
    #     params=model.get_grad_params(),
    #     lr=0.001,  # Learning rate
    #     betas=(0.9, 0.999),  # Beta1 and Beta2
    #     eps=1e-8,  # Epsilon
    # )
    _optimizer = Adam(params=_model.parameters(), **_optimizer_kwargs)

    # KBGAN use following parameters
    # Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)

    print("Loading negative sampler...")
    if config['neg_sampler'] == 'random':
        _negative_sampler = BasicNegativeSampler(mapped_triples=neg_sampler_triples.mapped_triples, num_negs_per_pos=config["num_negs"])
    elif config['neg_sampler'] == 'bernoulli':
        _negative_sampler = BernoulliNegativeSampler(mapped_triples=neg_sampler_triples.mapped_triples, num_negs_per_pos=config["num_negs"])
    else:
        _negative_sampler = WrapperNegativeSampler(
            mapped_triples=train_triples.mapped_triples,
            num_negs_per_pos=config.get("num_negs",1),
            config=config,
            ent_rel_idx_dir=config["dataset_path"],
            data_for_ns=data_for_negative_sampler
            )

    _training_loop = SLCWATrainingLoop(
        model=_model,
        triples_factory=train_triples,
        negative_sampler=_negative_sampler,
        optimizer=_optimizer,
        optimizer_kwargs=_optimizer_kwargs,
        # additional_filter_triples=[train_triples.mapped_triples,validation_triples.mapped_triples],
        #device=_device,
    )

    early_stopper = EarlyStopper(
        model=_model,
        evaluator=RankBasedEvaluator(),
        training_triples_factory= train_triples,
        evaluation_triples_factory=validation_triples,
        frequency=2,
        patience=10,
        relative_delta=0.002
        #result_tracker=ConsoleResultTracker()
    )

    train_start_time = time.time()
    print("***Training Started*** \t Time: {}".format(datetime.now()))
    losses_per_epoch = _training_loop.train(
        triples_factory=train_triples,
        num_epochs=config.get("num_epochs",1),
        batch_size=config.get("batch_size",100),
        clear_optimizer=True,  # Whether to delete the optimizer instance after training (as the optimizer might have additional memory consumption due to e.g. moments in Adam).
        # stopper=early_stopper, # Early stoppage crashes the code, it can safely run on Cuda device only
        
        # early_stopping=early_stopper, # eary stopper check: https://pykeen.readthedocs.io/en/v1.0.0/reference/training.html#pykeen.training.SLCWATrainingLoop.train
        
        # additional_filter_triples=[train_triples.mapped_triples,validation_triples.mapped_triples],
        #regularizer=_regularizer,
        # NEW: validation evaluation callback
        # callbacks="evaluation-loop", 
        use_tqdm=False,
        use_tqdm_batch=False,

        callbacks_kwargs=dict(
            prefix="validation",
            factory=validation_triples, # here the evaluation-loop is used for 
            ),
    )
    train_end_time = time.time()
    print("***Training Comlpeted*** \t Time: {}".format(datetime.now()))
    
    # total_training_time = "{}:{}:{}".format(*util.time_difference(train_start_time, train_end_time))
    total_training_time = train_end_time - train_start_time
    
    print("Writing trained model to directory: {}".format(config["results_dir"]))
    
    _model.save_state(os.path.join(config["results_dir"],"{}.model".format(config["exp_name"]))) # _model.load_state() will load the file and continue
    
    util.dump_json(losses_per_epoch, config["results_dir"], "{}_losses.json".format(config["exp_name"]))

    with open(config["results_dir"]+"/train_time_{}.txt".format(config["exp_name"]), "w") as file:
        # Write the message to the file
        file.write(str(total_training_time))

# region this callback function code is not in use    
    # Define a list to store training and evaluation results
    training_results = []
    evaluation_results = []

    def callback(epoch, training_loss, evaluation_loss):
        training_results.append(training_loss)
        evaluation_results.append(evaluation_loss)
        print("Epoch: ", epoch," : Training Loss: ", training_loss, " Evaluation Loss: ", evaluation_loss)

# endregion
    
    print("Model size in memory: {}".format(_model.num_parameter_bytes))
    # temp = _model.eval()
    # print("Model size in memory after cleaning: {}".format(sys.getsizeof(temp.num_parameter_bytes)))
    
    # evaluator = RankBasedEvaluator()
    evaluation_loop = LCWAEvaluationLoop(
        model=_model,
        evaluator=RankBasedEvaluator(),
        evaluator_kwargs=dict(
            filtered=True,
            ),
        triples_factory=test_triples,
        additional_filter_triples=[
            train_triples.mapped_triples,
            validation_triples.mapped_triples,
            ],
        #callbacks=callback,
    )

    print("***Evaluation Started (*** \t Time: {}".format(datetime.now()))  # there was an exception that's why code crashed before evaluation. now is not a property, it's a function and must be written as .now()
    eval_start_time = time.time()
    # results = evaluation_loop.evaluate(batch_size=1024)
    results = evaluation_loop.evaluate()
    eval_end_time = time.time()
    print("***Evaluation Ended (*** \t Time: {}".format(datetime.now()))
    # total_eval_time = "{}:{}:{}".format(*util.time_difference(eval_start_time, eval_end_time))
    total_eval_time = eval_end_time - eval_start_time


    results_dict = results.to_dict()
    #print(results_dict)

    util.dump_json(results_dict,config["results_dir"],"{}_metrics.json".format(config["exp_name"]))

    my_results_dic = dict(
        hit_at_1=results.get_metric('hits@1'),
        hit_at_3=results.get_metric('hits@3'),
        hit_at_5=results.get_metric('hits@5'),
        hit_at_10=results.get_metric('hits@5'),
        MRR=results.get_metric('mean_reciprocal_rank'),
        training_time=total_training_time,
        evaluation_time=total_eval_time,
    )

    util.dump_json(my_results_dic,config["results_dir"],"{}_metrics_hit_mrr.json".format(config["exp_name"]))

    print("Hits@1: ", results.get_metric('hits@1'))
    print("Hits@3: ", results.get_metric('hits@3'))
    print("Hits@5: ", results.get_metric('hits@5'))
    print("Hits@10: ", results.get_metric('hits@10'))
    print("Mean Reciprocal Rank: ", results.get_metric('mean_reciprocal_rank'))
    
    # # Print the evaluation results
    print(results)

import argparse

if __name__=='__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('exp_config_path')
    parser.add_argument('data_path')
    parser.add_argument('experiment_name')
    parser.add_argument('dataset_name')
    
    # parser.add_argument('--test_code', dest='is_code_testing', action='store_true', help='A boolean flag')
    # parser.add_argument('--test_dataset_name', type=str, default=None, help="An optional dataset argument (default is 'kinship')")
    # parser.add_argument('--test_model', dest='is_model_testing', action='store_true', help='A boolean flag')
    
    # "experiment_specs", "./data/", "FB15K", is_code_testing=False, is_model_test=False

    args = parser.parse_args()
    run_experiment(args.exp_config_path,args.data_path, args.experiment_name, args.dataset_name)
